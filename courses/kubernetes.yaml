module:
  name: k8spok-practice-course
  title: Kubernetes Practice
  description: Practical Kubernetes Tasks
  db_name: kubernetes
  courses:
  - course_id: 00-basics
    title: Kubernetes basic
    difficulty: beginner
    time: 45 minutes
    steps:
    - title: Namespaces
      task: |-
        In case you want to organize multiple users into separate teams projects, you can divide cluster into "sub-clusters" using **_Namespaces_**. The names of the resources/objects created inside a **Namespace** are unique, but not across Namespaces.

        ## Useful Commands:
        ```
        kubectl create -f namespace.yaml
        kubectl get namespaces
        kubectl get namespaces --show-labels
        kubectl get ns << NS_NAME >>
        kubectl explain namespace << NS_NAME >>
        kubectl describe ns << NS_NAME >>
        kubectl delete ns << NS_NAME >>
        ```

        ## Documentation:
        - https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/
      courseData: |-
        until kubectl get node master -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' | grep True; do sleep 1; done
        kubectl taint node master node-role.kubernetes.io/master-
        kubectl label node master node-role.kubernetes.io/worker=

        fruits=(apricot currant mango peach avocado guava lemon papaya orange tamarind)

        n=$(cat /dev/urandom | tr -dc '0-9' | fold -w 256 | head -n 1 | head --bytes 1)
        ns1="${fruits[n]}-$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 8 | head -n 1  | tr 'A-Z' 'a-z')"
        kubectl create ns ${ns1}
        kubectl label ns ${ns1} secret="santa-doesnt-like-this-fruit"

        n=$(cat /dev/urandom | tr -dc '0-9' | fold -w 256 | head -n 1 | head --bytes 1)
        ns2="${fruits[n]}-$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 8 | head -n 1  | tr 'A-Z' 'a-z')"
        kubectl create ns ${ns2}
        kubectl label ns ${ns2} secret="santa-likes-this-fruit"
      foreground: ""
      verify: ""

    - title: Creating and Deleting Namespaces
      task: |-
        ## Useful Commands:
        ```
        kubectl get ns
        kubectl create ns <<namespace>>
        kubectl delete ns <<namespace>>

        kubectl label <<resource type>> <<resource name>> <<key=value>>
        ```

        ## Tasks:
        1. Create Namespace `cucumber-prod`
        2. Delete Namespace `potato-dev`
        3. Create Namespace `{{ .StudentShort }}` and label it with the labels below:
            - `name={{ .StudentName }}`
            - `surname={{ .StudentSurname }}`


      courseData: |-
        kubectl create ns potato-dev
      foreground: ""
      verify: |-
        nn=$(echo ${STUDENT} | tr 'A-Z' 'a-z' | sed 's/\(.\).* /\1/')
        n1=$(echo ${STUDENT} | tr 'A-Z' 'a-z' | sed 's/ .*//')
        n2=$(echo ${STUDENT} | tr 'A-Z' 'a-z' | sed 's/.* //')

        kubectl get ns cucumber-prod > /dev/null 2>&1 &&
        [ "$(kubectl get ns 2>/dev/null | grep potato-dev)" == "" ] && 
        kubectl get ns $nn &&
        kubectl get ns $nn --show-labels | grep name=$n1 &&
        kubectl get ns $nn --show-labels | grep surname=$n2

    - title: Pods
      task: |-
        A Kubernetes pod is a group of containers with shared storage, network, and cgroup that are always scheduled to run on the same node. A pod is also the the smallest deployable unit of compute that can be created and managed by Kubernetes.

        ![A Pod](https://raw.githubusercontent.com/sbeliakou/mentor/master/.img/pods.png)

        ## Useful commands:
        ```
        kubectl get podd
        kubectl get pods -n <<namespace>>
        kubectl get pods <<pod_name>>
        kubectl get pods <<pod_name>> -n <<namespace>>
        kubectl get pods <<pod_name>> --all-namespaces
        kubectl get pods <<pod_name>> -o yaml
        kubectl get pods <<pod_name>> -o wide

        kubectl describe pods <<pod_name>> 
        kubectl describe pods <<pod_name>> -n <<namespace>> 

        kubectl apply -f <<pod-manifest.yaml>>
        kubectl delete -f <<pod-manifest.yaml>>

        kubectl run <<pod_name>> [-n <<namespace>>] --generator=run-pod/v1 --image=<<pod_image>>
        kubectl run <<pod_name>> [-n <<namespace>>] --generator=run-pod/v1 --image=<<pod_image>> -- --command <<container_command>>
        ```

        ## Let's explore what Pods are about
      courseData: |-
        kubectl create ns customer

        cat << EOF | kubectl apply -n customer -f -
        apiVersion: apps/v1
        kind: ReplicaSet
        metadata:
          name: abc
          labels:
            app: abc
        spec:
          replicas: 6
          selector:
            matchLabels:
              app: abc
          template:
            metadata:
              labels:
                app: abc
            spec:
              containers:
              - name: abc
                image: busybox
                command:
                - sleep
                - "10000"
        EOF
      foreground: ""
      verify: ""

    - title: Pods
      task: |-
        We've just created a pod.

        Let's explore it.
      courseData: kubectl run newpod --generator=run-pod/v1 --image=busybox --command sleep 1000
      foreground: ""
      verify: ""

    - title: Pods
      task: |-
        We just created a new POD. Ignore the state of the POD for now.

        You need to look at all the pods in detail to figure it out
      courseData: |-
        cat << EOF | kubectl apply -f -
        apiVersion: v1
        kind: Pod
        metadata:
          labels:
            run: stranger
          name: stranger
          namespace: default
        spec:
          containers:
          - name: billy
            image: busybox
            command:
            - sleep
            - "1000"
          - name: bobby
            image: agentx
            command:
            - sleep
            - "1000"
        EOF
      foreground: ""
      verify: ""

    - title: Creating a Pod
      task: |-
        Please find a pod definition in `/opt/practice/web-pod.yaml` and deploy it.

        Once it's done, please answer the following question:
      courseData: |-
        mkdir -p /opt/practice/
        kubectl create ns microservices
        cat << EOF > /opt/practice/web-pod.yaml
        apiVersion: v1
        kind: Pod
        metadata:
          labels:
            app: webserver
          name: webserver
          namespace: microservices
        spec:
          containers:
          - image: nginx
            name: webservice
        EOF
      foreground: ""
      verify: ""

    - title: Creating a Pod
      task: |-
        Create a new pod with the name '`redis`' and with the image '`redis:123`'
        And yes the image name is wrong!

        Create a pod-definition file.

        ## Requirements:
        - **Name**: `redis`
        - **Image Name**: `redis:123`
        - **Namespace**: `default`


      courseData: ""
      foreground: ""
      verify: |-
        kubectl get po -n default redis -o jsonpath='{.metadata.name}' | egrep '^redis$' && 
        kubectl get po -n default redis -o jsonpath='{.spec.containers[0].image}' | egrep '^redis:123$'

    - title: Pods
      task: |-
        Now change the image name on the pod to '`redis`'.
        Update the pod-definition file and use '`kubectl apply`' or '`kubectl edit pod redis`' commands.

        # Requirements:
        - **Name**: `redis`
        - **Image Name**: `redis:5.0.6-alpine3.10`
      courseData: ""
      foreground: ""
      verify: |-
        kubectl get po -n default redis -o jsonpath='{.metadata.name}' | egrep '^redis$' && 
        kubectl get po -n default redis -o jsonpath='{.spec.containers[0].image}' | egrep '^redis:5.0.6-alpine3.10$'

    - title: Deployments
      task: |-
        A Kubernetes **deployment** manages scheduling and lifecycle of pods. Deployments provide several key features for managing pods, including rolling out pods updates, rolling back, and easily scaling pods horizontally.

        ## Useful Commands:
        ```
        kubectl apply -f deployment.yaml
        kubectl get deploy
        kubectl get deploy --show-labels
        kubectl get deploy -n <<namespace>>
        kubectl get deploy -o yaml
        kubectl describe deploy -n <<namespace>>
        kubectl delete deploy <<namespace>>
        ```

      courseData: |-
        kubectl create ns simians
        echo << EOF > | kubectl apply -f -
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: crazymonkey
          namespace: simians
        spec:
          replicas: 12
          selector:
            matchLabels:
              name: crazymonkey
          template:
            metadata:
              labels:
                name: crazymonkey
            spec:
              containers:
              - name: main
                image: busybox888
                command: ["/bin/sh", "-c", "echo Hello ${STUDENT}! && sleep 3600"]
        EOF
      foreground: ""
      verify: ""

    - title: Deployment
      task: |-
        Let's try to fix our deployment. Its manifest is located here: `/opt/practice/monkeys.yaml`.
        There is an issue in the file, so try to fix it. And redeploy.

        Wait until all pods come up
      courseData: |-
        cat << EOF > /opt/practice/monkeys.yaml
        apiVersion: apps/v1
        kind: deployment
        metadata:
          name: crazymonkey
          namespace: simians
        spec:
          replicas: 12
          selector:
            matchLabels:
              name: crazymonkey
          template:
            metadata:
              labels:
                name: crazymonkey
            spec:
              containers:
              - name: main
                image: busybox888
                command: ["/bin/sh", "-c", "echo Hello ${STUDENT}! && sleep 3600"
        EOF
      foreground: ""
      verify: |-
        rs=$(kubectl get deployments -n simians crazymonkey -o jsonpath='{.status.replicas}')
        rrs=$(kubectl get deployments -n simians crazymonkey -o jsonpath='{.status.readyReplicas}')

        [ "$rs" == "12" ] &&
        [ "$rs" == "$rrs" ] &&
        kubectl get deployments -n simians crazymonkey -o jsonpath='{.spec.template.spec.containers[0].image}' | egrep '^busybox'

    - title: Service (SVC)
      task: |-
        Kubernetes Pods are mortal. They are born and when they die, they are not resurrected. If you use a Deployment to run your app, it can create and destroy Pods dynamically.

        Each Pod gets its own IP address, however in a Deployment, the set of Pods running in at the moment in time could be different from the set of Pods running that application a moment later.

        Simply saying, a Service aggregates pods at backend and acts as a LoadBalancer.

        Here's simple definition of the service:
        ```
        apiVersion: v1
        kind: Service
        metadata:
          name: echo-service
        spec:
          selector:
            app: << set the same label as deployment has >>
          ports:
            - protocol: TCP
              port: 80
              targetPort: 8080
        ```

        ## Task:
        - Deploy Echo Application from here: `/opt/practice/web-echo-deployment.yaml`
        - Deploy Echo Service from the snippet above

        ## Validation:
        To see if svc has own IP
        ```
        $ kubectl get svc echo-service
        NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
        echo-service   ClusterIP   10.102.161.86   <none>        80/TCP    13s
        ```

        To see if our svc (ep) has discovered backends
        ```
        $ kubectl describe ep echo-service 
        Name:         echo-service
        Namespace:    default
        Labels:       <none>
        Annotations:  endpoints.kubernetes.io/last-change-trigger-time: 2019-11-02T11:50:32Z
        Subsets:
          Addresses:          10.244.0.59,10.244.0.60,10.244.0.61,10.244.0.62,10.244.0.63
          NotReadyAddresses:  <none>
          Ports:
            Name     Port  Protocol
            ----     ----  --------
            <unset>  8080  TCP

        # Or simply this:
        $ kubectl get ep echo-service -o jsonpath='{.subsets[*].addresses[*].ip}'
        10.244.0.59 10.244.0.60 10.244.0.61 10.244.0.62 10.244.0.63
        ```

        To check what addresses our pods have (note: we use the same selector as svc):
        ```
        $ kubectl get pods -l app=echo-application -o wide
        NAME                       READY   STATUS    RESTARTS   AGE  IP            NODE     NOMINATED NODE   READINESS GATES
        web-echo-547479747-tgwhf   1/1     Running   0          1m   10.244.0.59   master   <none>           <none>
        web-echo-547479747-qtqph   1/1     Running   0          1m   10.244.0.60   master   <none>           <none>
        web-echo-547479747-fb65d   1/1     Running   0          1m   10.244.0.61   master   <none>           <none>
        web-echo-547479747-cqfc9   1/1     Running   0          1m   10.244.0.62   master   <none>           <none>
        web-echo-547479747-z8hjz   1/1     Running   0          1m   10.244.0.63   master   <none>           <none>
        ```

        ## Documentation
        - https://kubernetes.io/docs/concepts/services-networking/service/
      courseData: |-
        cat << EOF > /opt/practice/web-echo-deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: web-echo
          labels:
            app: echo-application
        spec:
          replicas: 5
          selector:
            matchLabels:
              app: echo-application
          template:
            metadata:
              labels:
                # POD label for selecting
                app: echo-application
            spec:
              containers:
              - name: main
                image: sbeliakou/web-echo:1
        EOF
      foreground: ""
      verify: |-
        rs=$(kubectl get deployment web-echo -o jsonpath='{.status.replicas}')
        rrs=$(kubectl get deployment web-echo -o jsonpath='{.status.readyReplicas}')
        pod_lbl=$(kubectl get deployment web-echo -o jsonpath='{.metadata.labels.app}')
        svc_lbl=$(kubectl get svc echo-service -o jsonpath='{.spec.selector.app}')

        [ $rs -eq $rrs ] && 
        [ "${pod_lbl}" == "${svc_lbl}" ]

    - title: Publishing Services (ServiceTypes)
      task: |-
        For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, that’s outside of your cluster.

        Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is `ClusterIP`.

        Type values and their behaviors are:
        - **ClusterIP**: Exposes the Service on a cluster-internal IP. This is the default ServiceType.
        - **NodePort**: Exposes the Service on each Node’s IP at a static port (the NodePort).
        - **LoadBalancer**: Exposes the Service externally using a CLOUD provider’s load balancer.

        ## Task:
        Expose Web-Echo Deployment to NodePort

        Example definition:
        ```yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: << svc-name >>
        spec:
          ports:
          - nodePort: << node port >>
            port: << svc port >>
            protocol: TCP
            targetPort: << pod port >>
          selector:
            << selecting rules >>
          type: NodePort
        ```

        ## Requirements:
        - **Svc Name**: `svc-echo-nodeport-30080`
        - **Svc Type**: `NodePort`
        - **Node Port**: `30080`
        - **Pod Target Port**: `8080`
        - **Pod Selector**: `app=echo-application`

        ## Documentation
        - https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
      courseData: ""
      foreground: ""
      verify: |-
        kubectl get svc svc-echo-nodeport-30080 -o jsonpath='{.spec.ports[*].nodePort}' | egrep '^30080$' &&
        curl localhost:30080 | grep 'hostname: web-echo'

    intro: |-
      # Getting Familiar with Kubernetes

      ![img](https://www.openshift.com/hubfs/images/enterprise-kubernetes.svg?t=1527106851625)

      ## This time we will learn following concepts:
      - Namespaces
      - Pods
      - Deployments
      - Services

      ### Let's start!

    finish: |-
      ## That's it for now.
    environment:
      hideintro: false
      showdashboards: true
      uilayout: terminal-iframe
      imageid: kubernetes
      dashboards:
        - name: User
          port: 8080
        - name: localhost
          port: 32768

  - course_id: 01-cluster
    title: Kubernetes cluster
    difficulty: beginner
    time: 45 minutes
    steps:
    - title: Kubernetes Control Plane
      task: |-
        ![cluster](https://raw.githubusercontent.com/sbeliakou/mentor/master/.img/control_plane.png)

        ## Task Description

        We're going to initialize Cluster Control Plane (Master) with `kubeadm` tool.

        The most common commands are:
        - `kubeadm init ...`
        - `kubeadm join ...`
        - `kubeadm token ...`

        Please follow the requirements and create the cluster.

        ## Requirements:
        - token: `abcdef.0123456789abcdef`
        - token life duration: `20m`
        - Pod Network CIDR: `10.244.0.0/16`

        ## Tips:
        - To initialize cluster control plane, please run following command `kubeadm init`. Please look for required options
        - You can destroy cluster configuration with `kubeadm reset cluster`
        - Use `--ignore-preflight-errors=all` to disable SWAP and IPTABLES setttings which fail preflight checks
        - You can find nearly created admin config file (to operate with Kubernetes cluster) in `/etc/kubernetes/`

        ## Verification:
        ```
        root@master:/# kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes
        kubectl get nodes
        NAME     STATUS   ROLES    AGE     VERSION
        master   Ready    master   35s     v1.15.3

        root@master:/# kubectl --kubeconfig=/etc/kubernetes/admin.conf get componentstatuses 
        NAME                 STATUS    MESSAGE             ERROR
        scheduler            Healthy   ok                  
        controller-manager   Healthy   ok                  
        etcd-0               Healthy   {"health":"true"}   
        ```

        ## Documentation:
        - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/
      courseData: ""
      foreground: ""
      verify: |-
        export KUBECONFIG=/etc/kubernetes/admin.conf

        kubectl get nodes >/dev/null 2>&1 &&
        kubeadm token list | grep abcdef.0123456789abcdef >/dev/null &&
        [ `kubeadm token list | grep abcdef.0123456789abcdef | awk '{print $2}' | egrep -c '[01][0-9]m'` -eq 1 ] &&
        echo done

    - title: You just should have seen following output
      task: |-

        ```
        root@master:/# kubectl --kubeconfig=/etc/kubernetes/admin.conf get componentstatuses 
        NAME                 STATUS    MESSAGE             ERROR
        scheduler            Healthy   ok                  
        controller-manager   Healthy   ok                  
        etcd-0               Healthy   {"health":"true"}   
        ```

        It shows statuses of all core Kubernetes componenets.<br>
        There's also `API Server`. But if you see the output as above it means API server works good.

        As you see, currently it's not really convenient to use extra option `--kubeconfig=/etc/kubernetes/admin.conf`. But this is the way how we can be authorized on the cluster as admins. We will find better way for using this file next level.

        To explore the infrastructure during the quiz, please use one of the following approaches:
        - `export KUBECONFIG=/etc/kubernetes/admin.conf`
        - `kubectl --kubeconfig=/etc/kubernetes/admin.conf ...`

        Let's take a look at the infrastructure we have got right now.
        ---
      courseData: ""
      foreground: ""
      verify: ""

    - title: Client Access to the Cluster - 1
      task: |-
        ## Requiremets:
        - user `root` should be able to use tool kubectl for managing k8s cluster (localhost)

        ## Verification:
        ```
        root@master:~$ kubectl get componentstatuses
        NAME                 STATUS    MESSAGE             ERROR
        controller-manager   Healthy   ok                  
        scheduler            Healthy   ok                  
        etcd-0               Healthy   {"health":"true"} 
        ```

        ## Documentation:
        - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/
      courseData: |-
        rm -rf ~/.kube/
      foreground: ""
      verify: |-
        kubectl get nodes >/dev/null 2>&1 &&
        echo done

    - title: Client Access to the Cluster - 2
      task: |-
        We have just created a user `user01`. Please do necessary configuration for this user so that it will be able to manage kubernetes cluster with `kubectl` tool

        ## Requiremets:
        - user `user01` should be able to use tool kubectl for managing k8s cluster (localhost)
        - Use `sudo` to switch between user accounts
        - User proper ownerships on required configuration file(s)

        ## Verification:
        ```
        user01@master:/# kubectl get componentstatuses 
        NAME                 STATUS    MESSAGE             ERROR
        controller-manager   Healthy   ok                  
        scheduler            Healthy   ok                  
        etcd-0               Healthy   {"health":"true"}
        ```

        ## Documentation:
        - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

      courseData: |-
        USERNAME=user01
        useradd ${USERNAME} -d /home/${USERNAME} -m -s /bin/bash
        echo "${USERNAME} ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/${USERNAME}
      foreground: ""
      verify: sudo -u user01 -i kubectl get nodes

    - title: Configuring `kubectl` bash Completion
      task: |-
        Configure `kubectl` bash completion for user `user01` permanently

        ## Verification:
        ```
        user01@master:/# kubectl <tab><tab>
        annotate       autoscale      cordon         drain          kustomize      port-forward   set            
        api-resources  certificate    cp             edit           label          proxy          taint          
        api-versions   cluster-info   create         exec           logs           replace        top            
        apply          completion     delete         explain        options        rollout        uncordon       
        attach         config         describe       expose         patch          run            version        
        auth           convert        diff           get            plugin         scale          wait  
        ```

        ## Documentation:
        - https://kubernetes.io/docs/tasks/tools/install-kubectl/#enabling-shell-autocompletion
        - https://kubernetes.io/docs/reference/kubectl/cheatsheet/#kubectl-autocomplete

      courseData: ""
      foreground: ""
      verify: sudo -u user01 -i /bin/bash -l -i -c 'type -t __start_kubectl' | grep function

    - title: Deploying POD Network Driver
      task: |-
        ![network](https://assets.digitalocean.com/articles/k8s-networking/double-service.png)


        ## Requirements:
        - Pod Network Manifest File is located here: `/opt/manifests/pod-network.yaml`


        ## Verification:
        ```
        kubectl get pods -n kube-system | grep kindnet
        kindnet-2rs42    1/1     Running   0          3m18s

        kubectl get nodes
        NAME     STATUS   ROLES    AGE     VERSION
        master   Ready    master   7m35s   v1.15.3
        ```

        ## Tips:
        - Check that CNI pods are running
      courseData: |-
        cat << EOF | kubectl apply -f -
        kind: ConfigMap
        apiVersion: v1
        metadata:
          name: coredns
          namespace: kube-system
        data:
          Corefile: |
            .:53 {
                errors
                health
                kubernetes cluster.local in-addr.arpa ip6.arpa {
                   pods insecure
                   fallthrough in-addr.arpa ip6.arpa
                   ttl 30
                }
                forward . /etc/resolv.conf
                cache 30
                reload
                loadbalance
            }
        EOF

        # kubectl patch deployments.apps -n kube-system coredns -p '{
        #   "spec": {
        #     "replicas": 1, 
        #     "template": {
        #       "spec": {
        #         "containters": [
        #           {
        #             "name": "coredns", 
        #             "nodeSelector": {
        #               "kubernetes.io/hostname": "master"
        #             }
        #           }
        #         ]
        #       }
        #     }
        #   }
        # }'

        kubectl rollout restart deployment -n kube-system coredns

        mkdir -p /opt/manifests/
        cat /kind/manifests/default-cni.yaml | sed 's/.. .PodSubnet ../10.244.0.0\/16/' > /opt/manifests/pod-network.yaml
      foreground: ""
      verify: |-
        kubectl get pods -n kube-system | grep kindnet | grep Running >/dev/null && 
        kubectl get nodes master -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' | grep True

    - title: Lost bootstrap token 
      task: |-
        Unfortunately token created during cluster initialization was lost.
        Create a new bootstrap token.
        (It's used for establishing trust with server. We will use it on the next step.)


        ## Tips:
        - token `abcdef.0123456789abcdef` was lost;
        - create new `fedcba.fedcba9876543210` token;
        - use `kubeadm token list` for checking.


        ## Documentation:
        - https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/
        - https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-token/

      courseData: kubeadm token delete abcdef.0123456789abcdef
      foreground: ""
      verify: [[ $(kubeadm token list | grep -c fedcba.fedcba9876543210) -eq 1 ]]

    - title: Joining `node01` to the cluster 
      task: |-
        ![worker](https://raw.githubusercontent.com/sbeliakou/mentor/master/.img/cluster.png)

        ## Task:
        - Join worker node `node01` to the cluster
        - Make sure Kubernetes service will start after system reboot automatically

        Command to be used (on node01):
        ```
        kubeadm join <arguments>
        ```

        ## Tips:
        - Remember, token should be `fedcba.fedcba9876543210`?
        - Default API Port on `master` is `6443`
        - Use `ssh node01` to connect to `node01` host
        - Use also `--discovery-token-unsafe-skip-ca-verification` and  `--ignore-preflight-errors=all` options
        - Wait till `node01` turns to `Ready` state

        ## Verification:
        ```
        root@master:/# kubectl get nodes
        NAME     STATUS   ROLES    AGE   VERSION
        master   Ready    master   39m   v1.15.3
        node01   Ready    <none>   70s   v1.15.3
        ```

        ## Documentation:
        - https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/


      courseData: ""
      foreground: ""
      verify: |-
        kubectl get nodes node01 -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' | grep True &&
        ssh node01 'systemctl list-unit-files' | grep enabled | grep kubelet.service

    - title: Labeling Resources
      task: |-
        ## Requirements:
        - Label *node01* node as `worker`

        ```
        kubectl get nodes
        NAME     STATUS   ROLES      AGE     VERSION
        master   Ready    master     3m43s   v1.15.3
        node01   Ready    worker     46s     v1.15.3
        ```

        ## Tips:
        - You should just label necessary node as `node-role.kubernetes.io/<< node role >>`
        - Use commande like `kubectl label node <nodename> <labelname>=<labelvalue>`
        - If you need to label resource with empty label just ommit `<labelvalue>`

        ## Documentation:
        - http://kubernetesbyexample.com/labels/
        - https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#updating-labels
        - https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#label

      courseData: ""
      foreground: ""
      verify: kubectl get nodes node01 -o json | jq '.metadata.labels."node-role.kubernetes.io/worker"' | grep '""'

    - title: Kubernetes Dashboard
      task: |-
        ![dashboard](https://github.com/kubernetes/dashboard/raw/master/docs/images/dashboard-ui.png)

        ## Requirements:
        - Please use following manifest: [kubernetes-dashboard.yaml on github](https://github.com/kubernetes/dashboard/blob/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml)

        ## Guide:

        ### Create Admin User

        ```
        kubectl create serviceaccount admin-user -n kube-system
        kubectl create clusterrolebinding admin-user-clusterrolebinding \
          --serviceaccount=kube-system:admin-user \
          --clusterrole=cluster-admin
        ```

        ### Get Admin Secret

        ```
        SECRET_NAME="$(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')"
        kubectl get secret -n kube-system ${SECRET_NAME} -o jsonpath='{.data.token}' | base64 -d
        ```

        ### Proxy Dashboard

        Expose Dashord from inside cluster with `kubectl proxy` command:

        ```
        kubectl proxy --address='0.0.0.0' --accept-hosts='^*$'
        ```

        ### Explore Kubernetes Cluster on "Dashboard" Tab

        Please use token authentication from previous step

        ### Documentation:
        - https://github.com/kubernetes/dashboard
        - https://github.com/kubernetes/dashboard/wiki/Creating-sample-user

      courseData: ""
      foreground: ""
      verify: |-
        [ `kubectl get deployment -n kube-system kubernetes-dashboard -o jsonpath='{.spec.template.spec.containers[0].name}'` == "kubernetes-dashboard" ] &&
        [ `kubectl get svc -n kube-system kubernetes-dashboard -o jsonpath='{.spec.ports[0].targetPort}'` == "8443" ] &&
        [ `kubectl get ep -n kube-system kubernetes-dashboard -o jsonpath="{.subsets[*].addresses[0].nodeName}"` == 'node01' ] &&
        echo done

    - title: Deploying Metrics Server
      task: |-
        ![metrics](https://user-images.githubusercontent.com/21168270/46579266-95846680-ca40-11e8-86d3-a42291476db8.png)


        ## Task:

        Follow the [documentation steps](https://github.com/kubernetes-incubator/metrics-server) and deploy `metrics-server`.

        You should update update metrics-server Deployment configuration so that it starts working correctly in your cluster:
        ```
        root@master:~# kubectl edit deployments -n kube-system metrics-server
        ...
        apiVersion: extensions/v1beta1
        kind: Deployment
        ...
              containers:
              - image: k8s.gcr.io/metrics-server-amd64:v0.3.6
                imagePullPolicy: Always
                name: metrics-server
                command:
                - /metrics-server
                - --kubelet-preferred-address-types=InternalIP,Hostname,ExternalIP
                - --kubelet-insecure-tls
        ...
        ```

        ## Cautions:
        - It usually takes a few minutes to start responding with metrics statistics, so don't worry.
        - If Kubernetes responds with `Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)` or `error: metrics not available yet` just wait for a while

        ## Check that:
        - Metrics Server is deployed successfully and is `Running`
        - Command `kubectl top nodes` shows performance statistics by nodes
        - Command `kubectl top pods --all-namespaces` shows performance statistics by pods

        ## Documentation:
        - https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/
        - https://github.com/kubernetes-incubator/metrics-server
        - https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#top

      courseData: ""
      foreground: ""
      verify: |-
        kubectl get deployment -n kube-system metrics-server -o jsonpath='{.status.readyReplicas}' | grep 1 &&
        kubectl top pods &&
        kubectl top nodes

    - title: Deploying Ingress-Controller
      task: |-
        ![ingress](https://raw.githubusercontent.com/sbeliakou/mentor/master/.img/ingress.png)


        ## Requirements :
        - [Ingress-controller manifest](https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml)
        - Ingress-controller Service Manifest: `/opt/manifests/ingress-svc.yaml`


        ## Please be patient. It may take a while ...

        ## Verification:  
        - Make sure **nginx-ingress-controller-...** Pod is running in **ingress-nginx** namespace. Also you should see **ingress-nginx** service in **ingress-nginx** namespace.  
        - Explore `localhost` page in your browser. You should see nginx ingress controller default page (*etc. openresty*). 
        - Explore `localhost/green` page in your browser. You should see green color page.  
          

        ## Documentation:
        - https://kubernetes.io/docs/concepts/services-networking/ingress/
        - https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/
        - https://kubernetes.github.io/ingress-nginx/
      courseData: |-
        kubectl run allgood --generator=run-pod/v1 --image=sbeliakou/stubserver --port=80
        kubectl expose pod allgood --name=allgood-svc --port=80

        cat << EOF | kubectl apply -f-
        apiVersion: networking.k8s.io/v1beta1
        kind: Ingress
        metadata:
          name: allgood-ingress
          annotations:
            nginx.ingress.kubernetes.io/rewrite-target: /
        spec:
          rules:
          - http:
              paths:
              - path: /allgood
                backend:
                  serviceName: allgood-svc
                  servicePort: 80
        EOF


        mkdir -p /opt/manifests/
        cat << EOF > /opt/manifests/ingress-svc.yaml
        kind: Service
        apiVersion: v1
        metadata:
          name: ingress-nginx
          namespace: ingress-nginx
          labels:
            app.kubernetes.io/name: ingress-nginx
            app.kubernetes.io/part-of: ingress-nginx
        spec:
          externalIPs:
          - 172.31.0.2
          ports:
          - name: http
            port: 80
            protocol: TCP
            targetPort: http
          selector:
            app.kubernetes.io/name: ingress-nginx
            app.kubernetes.io/part-of: ingress-nginx
          sessionAffinity: None
          type: ClusterIP
        EOF

      foreground: ""
      verify: |-
        [[ $(kubectl get deployments -n ingress-nginx nginx-ingress-controller -o jsonpath='{.status.readyReplicas}') == '1' ]] &&
        echo done

    intro: |-
      # Cluster Creation with kubeadm

      ## This time we will do the next
      - Initialize Kubernetes Control Plain
      - Configure kubectl utility
      - Depploy POD Network
      - Join Worker Node
      - Decorate Worker Node with Custom Label
      - Deploy Kubernetes Dashboard
      - Deploy Metrics-server

      ### Let's start!

    finish: |-
      # Summary

      We have created Kubernetes cluster of two nodes, configured client access, deployed dashboard, ingress-controller and metrics server.
      Some useful commands you should capture:

      ```
      kubectl get nodes
      kubectl cluster-info
      kubectl get componentstatus

      kubectl get ns ...
      kubectl get po ...
      kubectl get deployments ...
      kubectl describe po ...
      kubectl describe deployments ...

      kubectl top nodes
      kubectl top pods 
      kubectl top pods --all-namespaces
      ```
    environment:
      hideintro: false
      showdashboards: true
      uilayout: terminal-iframe
      imageid: kubernetes
      dashboards:
        - name: User
          port: 8080
        - name: localhost
          port: 32768

  - course_id: 02-pods
    title: Kubernetes Pods
    difficulty: beginner
    time: 45 minutes
    steps:
    - title: Create a simple Pod
      task: |-
        ## Requirements:
        - **Pod name**: `nginx-pod`
        - **Pod image**: `nginx:alpine`
        - **Pod label**: `app=nginx`
        - **Namespace**: `default`


        Wait till this pod comes into `Running` state

        ## Documentation:
        - https://kubernetes.io/docs/concepts/workloads/pods/pod/
        - https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/#pod-templates
      courseData: |-
        until kubectl get node master -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' | grep True; do sleep 1; done
        kubectl taint node master node-role.kubernetes.io/master-
        kubectl label node master node-role.kubernetes.io/worker=
      foreground: ""
      verify: |-
        [ `kubectl get pods nginx-pod -o jsonpath='{.status.phase}'` == "Running" ] &&
        [ `kubectl get pods nginx-pod -o jsonpath='{.spec.containers[0].image}'` == "nginx:alpine" ] &&
        [ `kubectl get pods nginx-pod -o jsonpath='{.metadata.labels.app}'` == "nginx" ]

    - title: Troubleshooting Pods
      task: A new Pod `web` has been deployed. It failed. 
        Please fix it.

        ## Requirements:
        - **Image**: `nginx` of `v1.16.1` based on `alpine`
        - **Pod Status**: `Running`

        ## Documentation:
        - https://kubernetes.io/docs/concepts/workloads/pods/pod/
      courseData: |-
        kubectl create ns webservices &&
        kubectl run web --image=nginx:alpine-1.16 --generator=run-pod/v1 -n webservices
      foreground: ""
      verify: |-
        [ `kubectl get pods web -n webservices -o jsonpath='{.status.phase}'` == "Running" ] &&
        [ `kubectl get pods web -n webservices -o jsonpath='{.spec.containers[0].image}'` == "nginx:1.16.1-alpine" ]

    - title: Troubleshooting Pods
      task: |-
        A new Pod `redis-db` has been deployed. It failed. 
        Please fix it.

        If it's required you can recreate a pod. But be careful.

        Wait till this pod comes into `Running` state.

        ## Documentation:
        - https://kubernetes.io/docs/concepts/workloads/pods/pod/
        - https://kubernetes.io/docs/concepts/workloads/pods/init-containers/

      courseData: |-
        kubectl create ns db
        cat << EOF | kubectl apply -n db -f-
        apiVersion: v1
        kind: Pod
        metadata:
          name: redis-db
        spec:
          containers:
          - image: redis:alpine
            name: redis-db
          initContainers:
          - image: busybox
            name: starter
            command: 
            - sleeeep
            - "10"
          restartPolicy: Never
        EOF
      foreground: ""
      verify: |-
        [ `kubectl get pods -n db redis-db -o jsonpath='{.status.phase}'` == "Running" ] &&
        [ `kubectl get pods -n db redis-db -o jsonpath='{.spec.initContainers[0].command[0]}'` == "sleep" ] &&
        [ `kubectl get pods -n db redis-db -o jsonpath='{.spec.initContainers[0].image}'` == "busybox" ] &&
        [ `kubectl get pods -n db redis-db -o jsonpath='{.spec.containers[0].image}'` == "redis:alpine" ]

    - title: Creating a Pod
      task: |-
        Create a new pod with the name '`redis`' and with the image '`redis:123`'
        And yes the image name is wrong!

        Create pod-definition file.

        ## Requirements:
        - **Name**: `redis`
        - **Image Name**: `redis:123`
        - **Namespace**: `default`

      courseData: ""
      foreground: ""
      verify: |-
        kubectl get po -n default redis -o jsonpath='{.metadata.name}' | egrep '^redis$' && 
        kubectl get po -n default redis -o jsonpath='{.spec.containers[0].image}' | egrep '^redis:123$'

    - title: Pods
      task: |-
        Now fix the image on the pod to '`redis`'.
        Update the pod-definition file and use '`kubectl apply`' command or use '`kubectl edit pod redis`' command.

        # Requirements:
        - **Name**: `redis`
        - **Image Name**: `redis:5.0.6-alpine3.10`
      courseData: ""
      foreground: ""
      verify: |-
        kubectl get po -n default redis -o jsonpath='{.metadata.name}' | egrep '^redis$' && 
        kubectl get po -n default redis -o jsonpath='{.spec.containers[0].image}' | egrep '^redis:5.0.6-alpine3.10$'

    - title: Static Pods
      task: |-
        Create static Pod.

        ## Requirements:
        - **Pod Name**: `static-nginx` (might be automatically added `-<<node-name>>` postfix in `kubectl get pods`)
        - **Image**: `nginx`

        For self-checking and discovering of static Pod's mechanism try to delete static Pod using `kubectl`. 

        ## Documentation:
        - https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/
      courseData: ""
      foreground: ""
      verify: |-
        [ "$(kubectl get pods static-nginx-master -o jsonpath='{.status.phase}')" == "Running" ] &&
        grep static-nginx /etc/kubernetes/manifests/*

    intro: |-
      # Practicing with Pods

      Here we have collected a number of scenarios which will help you to get more experienced working with Pods.

      - Creating Pods
      - Learming Init Containers
      - Troubleshooting various issues with Pods
      - Getting familiar with how Kubernetes manages this resources

      ### Here we go

    finish: |-
      # Here we are
    environment:
      hideintro: false
      showdashboards: true
      uilayout: terminal-iframe
      imageid: kubernetes
      dashboards:
        - name: User
          port: 8080
        - name: localhost
          port: 32768

  - course_id: 03-deployments
    title: Kubernetes Deployments
    difficulty: beginner
    time: 45 minutes
    steps:
    - title: Create Deployment
      task: |-
        Create a new deployment called `nginx-deploy`:

        ## Requirements:
        - **Name**: `nginx-deploy`
        - **Image**: `nginx:1.16-alpine`
        - **Replicas**: `3`
        - Make sure that pods are running

        ## Documentation:
        - https://kubernetes.io/docs/concepts/workloads/controllers/deployment/

      courseData: |-
        until kubectl get node master -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' | grep True; do sleep 1; done
        kubectl taint node master node-role.kubernetes.io/master-
        kubectl label node master node-role.kubernetes.io/worker=
      foreground: ""
      verify: |-
        [ `kubectl get deployment nginx-deploy -o jsonpath='{.metadata.name}'` == "nginx-deploy" ] && 
        [ `kubectl get deployment nginx-deploy -o jsonpath='{.spec.template.spec.containers[0].image}'` == "nginx:1.16-alpine" ] && 
        [ `kubectl get deployment nginx-deploy -o jsonpath='{.spec.replicas}'` == 3 ] &&
        [ `kubectl get deployment nginx-deploy -o jsonpath='{.status.readyReplicas}'` == 3 ]

    - title: Deleting Pods
      task: |-
        Check our pods of this deplyment.<br>Please, delete one of pods and let's check what heppens.

        ReplicaSet which is creatted by deplyment will create new one. Right?

        ## Task:
        - Delete one of pods and verivy that the new one has been created.
      courseData: ""
      foreground: ""
      verify: |-
        rs=$(kubectl get rs | grep nginx-deploy | awk '{print $1}')
        kubectl get events | grep replicaset/$rs | wc -l | xargs -IF [ F -gt 3 ]

    - title: Updating Deployment
      task: |-
        Use rolling upgrade to provide it with the `1.17` version of `nginx:alpine`.<br>Please check ReplicaSets status.

        ## Requirements:
        - current deployment image is `nginx:1.16-alpine`
        - new deployment has been upgraded to version `1.17-alpine` using rolling update

        ## Commands:
        - `kubectl set image <<resource>> <<resource name>> <<contanier>>=<<new image>>`
        - `kubectl describe <<resource>> <<resource name>>`
        - `kubectl get rs <<replicaset name>>`

        ## Note:
        - You can easily roll back with the command `kubectl rollout undo <<resource>> <<deployment name>>`

        ## Verification:
        - There should be 2 replicas like here:
        > ```
        > kubectl get rs -o wide
        > NAME                      DESIRED   CURRENT   READY   ...   IMAGES              ...
        > nginx-deploy-5ffcd8bc7f   0         0         0       ...   nginx:1.16-alpine   ...
        > nginx-deploy-698fdf9b46   3         3         3       ...   nginx:1.17-alpine   ...
        > ```

        ## Documentation:
        - https://kubernetes.io/docs/concepts/workloads/controllers/deployment/

      courseData: ""
      foreground: ""
      verify: |-
        rs16=$(kubectl get rs -o wide | grep 'nginx:1.16-alpine' | awk '{print $1}')
        rs17=$(kubectl get rs -o wide | grep 'nginx:1.17-alpine' | awk '{print $1}')

        [ -n "$rs16" ] && [ -n "$rs17" ] &&
        kubectl get rs | grep -v NAME | wc  -l | grep '^2$' &&
        [ $(kubectl get rs/${rs17} -o jsonpath='{.status.availableReplicas}') -eq 3 ]

    - title: Rolling Back
      task: |-
        Now please roll back to previous version of the deployment: `nginx:1.17-alpine`

        ## Documentation:
        - https://kubernetes.io/docs/concepts/workloads/controllers/deployment/

      courseData: ""
      foreground: ""
      verify: |-
        rs16=$(kubectl get rs -o wide | grep 'nginx:1.16-alpine' | awk '{print $1}')
        rs17=$(kubectl get rs -o wide | grep 'nginx:1.17-alpine' | awk '{print $1}')

        [ -n "$rs16" ] && [ -n "$rs17" ] &&
        kubectl get rs | grep -v NAME | wc  -l | grep '^2$' &&
        [ $(kubectl get rs/${rs16} -o jsonpath='{.status.availableReplicas}') -eq 3 ]

    - title: Troubleshooting Deployments
      task: |-
        New deployment has been created, but it doesn't work properly. Find out and fix the issue.

        ## Requirements:
        - Deployment `orange` is running
        - Pod, associated with this deployment, is up and running
        - Pod waits for 10 seconds before creating a container
        - Please, change only necessary parameters

        ## Documentation:
        - https://kubernetes.io/docs/concepts/workloads/controllers/deployment/

      courseData: |-
        kubectl create ns orange &&
        cat << EOF | kubectl apply -n orange -f-
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          namespace: orange
          labels:
            run: orange
          name: orange
        spec:
          replicas: 2
          selector:
            matchLabels:
              run: orange
          template:
            metadata:
              labels:
                run: orange
            spec:
              containers:
              - image: nginx:1.16-alpine
                name: orange
              initContainers:
              - image: busybox:alpine
                name: busybox
                command:
                - s11eep
                - "10"
        EOF
      foreground: ""
      verify: |-
        [ `kubectl get deployment orange -n orange -o jsonpath='{.metadata.name}'` == "orange" ] && 
        [ `kubectl get deployment orange -n orange -o jsonpath='{.spec.template.spec.containers[0].image}'` == "nginx:1.16-alpine" ] && 
        [ `kubectl get deployment orange -n orange -o jsonpath='{.spec.replicas}'` == 2 ] &&
        [ `kubectl get deployment orange -n orange -o jsonpath='{.status.readyReplicas}'` == 2 ] &&
        [[ `kubectl get deployment orange -n orange -o jsonpath='{.spec.template.spec.initContainers[0].image}'` =~ "busybox" ]] &&
        [ `kubectl get deployment orange -n orange -o jsonpath='{.spec.template.spec.initContainers[0].command[1]}'` == 10 ]

    - title: Troubleshooting Deployments
      task: |-
        New deployment has been created, but there are no `replcaSet`s associated with it. Find out and fix the issue

        ## Requirements:
        - deployment `lemon` is up and running
        - deployment has 1 `replicaSet` associated with it
        - there are 3 pods managed by this replicaSet

        ## Documentation:
        - https://kubernetes.io/docs/concepts/workloads/controllers/deployment/

      courseData: |-
        kubectl create ns lemon &&
        cat << EOF | kubectl apply -n lemon -f-
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          namespace: lemon
          labels:
            run: lemon
          name: lemon
        spec:
          replicas: 0
          selector:
            matchLabels:
              run: lemon
          template:
            metadata:
              labels:
                run: lemon
            spec:
              containers:
              - image: nginx
                name: lemon
        EOF
      foreground: ""
      verify: ""

    - title: Troubleshooting Deployments
      task: |-
        There's manifest `/opt/practice/tomato.yaml`. It needs some key details. Please add necessary configuration and deploy it.
      courseData: |-
        kubectl create ns tomato
        mkdir /opt/practice/
        cat << EOF > /opt/practice/tomato.yaml
        apiVersion: apps/v1
        kind: deployment
        metadata:
          namespace: tomato
          labels:
            vegetable: tomato
          name: tomato
        spec:
          replicas: 1
          template:
            spec:
              containers:
              - image: nginx:1.16-alpine
                name: tomato
        EOF
      foreground: ""
      verify: |-
        kubectl get deployments. -n tomato tomato -o jsonpath='{.status.readyReplicas}' | grep '^1$' &&
        kubectl get deployments. -n tomato tomato -o jsonpath='{.metadata.labels.vegetable}' | grep '^tomato$'

    intro: |-
      # In this section you will deal with Deployments:

      - Creating Deployments
      - Troubleshooting various issues with Deployments
      - Getting familiar with how Kubernetes manages this workload

      ### Good Luck!

    finish: |-
      # Here we are
    environment:
      hideintro: false
      showdashboards: true
      uilayout: terminal-iframe
      imageid: kubernetes
      dashboards:
        - name: User
          port: 8080
        - name: localhost
          port: 32768

  # - course_id: 04-services
  #   title: Kubernetes Services
  #   difficulty: beginner
  #   time: 45 minutes
  #   steps:


















  # - course_id: 01-images
  #   title: Docker images
  #   difficulty: beginner
  #   time: 45 minutes
  #   steps:
  #   - title: _title
  #     task: _task
  #     courseData: ""
  #     foreground: ""
  #     verify: ""
  #   intro: _intro
  #   finish: _finish
  #   environment:
  #     hideintro: false
  #     showdashboards: true
  #     uilayout: terminal-iframe
  #     imageid: kubernetes
  #     dashboards:
  #       - name: User
  #         port: 8080
  #       - name: localhost
  #         port: 32768